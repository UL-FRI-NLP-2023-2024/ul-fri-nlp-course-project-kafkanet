# NLP course 2023/24: KafkaNet

<p align="center">
    <img src="figures/franc.jpg" width="200px"><br>
    <em style="font-family: Georgia;">
        ðŸ§™ Conversations with Characters in Stories for Literacy
    </em>
</p>

## ðŸª³ Project description

This repository contains the code and data for the project **Conversations with
Characters in Stories for Literacy** for the *Natural language processing*
course in the year 2023/24 at the *Faculty of Computer and Information Science*
in Ljubljana.

We would like to create quick, customized personal bots from novels. Currently,
there is a world-wide literacy crisis, because young people are reading less
and less, even though literacy is vital for educational and professional
success. We would like to motivate young people to read through conversational
interactions with digital pedagogical agents in the form of persona bots. One
such solution can be provided by large language models (LLMs). There are
solutions available, but we would like to provide a model suitable for
educational purposes.

We use pre-trained LLMs with prompt engineering methods like RAG to provide
knowledge to the models without additional training. Along with the book
content, we also provide a system prompt which tells the model to answer like
a character from the given book. The models we are comparing are:

- ChatGPT,
- Mistral,
- Llama 3,
- Llama 3 with large context (Gradient),
- LlamaIndex (RAG).

### Team members

- BlaÅ¾ Erzar
- Luka Salvatore Pecoraro
- Jakob Adam Å ircelj

## Repository organization

The repository is organized into directories as follows:

- `src`: Scripts and code for the project.
- `report`: The LaTeX source code and output PDF for the project report.
- `figures`: Figures generated by the project.
- `answers`: Responses of the models to the prompts.

For reproducibility, we provide `requirements.txt` with the necessary Python
packages and versions.

## Results

All the models were evaluated using the same system prompt. The system prompt
optimization steps can be seen in the [system.txt](answers/system.txt) file.
We evaluated the models responses for four books ([answers](answers)) manually
on three different criteria:

- roleplay,
- factuality and
- engagement.

We evaluated each model from best (4) to worst (1) in each category for each
book. To get the total score for each category and overall, we use the Borda
count method. The final scores are shown in the plot below.

<p align="center">
    <img src="figures/rank_count_final.png" width="400px" />
</p>
